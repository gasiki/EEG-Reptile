{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import EEGReptile as er   # import Reptile-EEG lib\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3378a2d3ffe654b0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prepare data and saving folders for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e533f7c34f3dd09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "experiment_name = er.experiment_storage(experiment_name='BCI_IV_EEGNet_example')   #create storage for models and new experiment data\n",
    "dataset = er.MetaDataset(dataset_name='BCI_IV')   #load dataset created in load data example\n",
    "print('Dataset ' + dataset.dataset_name + ' loaded.' + ' subjects: ' + str(dataset.subjects))\n",
    "targ = [7] # choose target subject to perform tests (Zero-. Few-shot) later\n",
    "tr_sub = copy.deepcopy(dataset.subjects)\n",
    "for sub in targ:   # remove target subject from \n",
    "    tr_sub.remove(sub) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c980b396500963",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a071e8ee330c0066",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "mparams = {   # params of EEGNet model\n",
    "    \"model_type\": \"inEEG_net\",\n",
    "    \"num_classes\": 4,\n",
    "    \"dropout1\": 0.52,\n",
    "    \"dropout2\": 0.36,\n",
    "    \"f1\": 16,\n",
    "    \"sampling_rate\": 250,\n",
    "    \"num_channels\": 22,\n",
    "    \"depth_multiplier\": 12,\n",
    "    \"time_of_interest\": 500,\n",
    "    \"time_points\": 625,\n",
    "    \"lowpass\": 50,\n",
    "    \"point_reducer\": 5\n",
    "}\n",
    "model = er.model_from_params(mparams) # crate model with specified params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e4a4c6958e985",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Find the meta-learning hyperparameters\n",
    "`\n",
    "meta_params(metadataset: MetaDataset, model, tr_sub: list, tst_sub, trials, jobs, mode='single_batch',\n",
    "                double_meta_step=False, meta_optimizer=False, experiment_name='experiment')\n",
    "` \n",
    "meta_params - function used for meta hyper-params search\n",
    "- **metadataset**: this is working dataset\n",
    "- **tr_sub**: list of subjects used for training in params search\n",
    "- **tst_sub**: list of subjects or None used for testing ACC in params search if none 2-k fold of tr_sub is used\n",
    "- **model**: model for which params search will be performed\n",
    "- **trials**: number of optuna trials in param search\n",
    "- **jobs**: how many parallel jobs to use in params search\n",
    "- **mode**: mode of meta train, may be: single_batch, batch or epoch, single_batch is more time efficient\n",
    "- **double_meta_step**: boolean flag for double meta step (for supported NN)\n",
    "- **meta_optimizer**: boolean flag for meta optimizer (Adam)\n",
    "- **experiment_name**: str experiment name from the experiment_storage function\n",
    "- **return**: dict of params for meta training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1978c8577d353f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "params = er.meta_params(metadataset=dataset, model=model, tr_sub=tr_sub, trials=100,\n",
    "                        experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b1b8071d5d06f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Perform meta-training (pretrained models will be saved in experiment folder)\n",
    "`\n",
    "meta_exp(params: dict, model, target_sub: list, metadataset: MetaDataset, mode='single_batch',\n",
    "             meta_optimizer=False, num_workers=1, experiment_name='experiment', all_subjects: list = None,\n",
    "             baseline=True, early_stopping=0)\n",
    "`\n",
    "**meta_exp** - function for meta learning and baseline transfer learning\n",
    "- **params** : dict, params for meta-learning from meta_params function\n",
    "- **model** : model for which params search will be performed\n",
    "- **target_sub** : list, target subjects for meta training (each will have a meta-trained model from which training this subject will be excluded)\n",
    "- **metadataset** : MetaDataset, name of working dataset\n",
    "- **mode**: \n",
    "- - 'single_batch' - only one batch per meta epoch size of **params[in_datasamples]**\n",
    "- - 'batch' - all training data for each meta epoch in batches size of **params[in_datasamples]**\n",
    "- - 'epoch' - all training data for each meta epoch in one batch,\n",
    "-  **meta_optimizer** boolean flag for meta optimizer (Adam)\n",
    "- **num_workers** : int, specifies number of threads for meta-learning experiments (usually 1 or 2), \n",
    "- **experiment_name** : str, experiment name from the experiment_storage function\n",
    "- **all_subjects** : list or None, list of all subjects for meta-training if None all subjects from dataset will be used,\n",
    "- **baseline** : boolean, prepare or not baseline model pretrained with transfer-learning\n",
    "- **early_stopping** : int, meta-learning early stopping after **int** epochs without improvement if 0 - disabled\n",
    "\n",
    "**For meta-training one neural network use function:**\n",
    "`meta_train(params: dict, model, metadataset: MetaDataset, wal_sub, path, name: str = None,\n",
    "               mode='single_batch', meta_optimizer=False, subjects: list = None, loging=True, baseline=True,\n",
    "               early_stopping=0)`\n",
    "- **wal_sub** : int, id of subject for validation\n",
    "- **path** : str, where to save meta-trained model\n",
    "- **name** : str, name for model weights file (if None will be **<wal_sub>_reptile.pkl**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61b0d744757744",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pretraining_auc = er.meta_exp(params=params, model=model, target_sub=targ + [3], # here we add subj 3 to use him in parameters search for fine-tuning \n",
    "                              metadataset=dataset, mode='batch', meta_optimizer=False, num_workers=2,\n",
    "                              experiment_name=experiment_name,\n",
    "                              baseline= True)  # train or not similar network on same data with classic Transfer Learning\n",
    "print('Pretraining completed. Mean auc for pretraining: ' + str(pretraining_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1074eb298ddcc72",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Find hyperparams for fine-tuning\n",
    "- **lr**\n",
    "- (**a**x + **b**) = Epochs, where x is number of datapoints for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b471eeb344608e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "af_params = er.aftrain_params(metadataset=dataset, model=model, tst_subj=[3], trials=100, jobs=2,\n",
    "                              experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6dac82bd487d4e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Fine-tuning experiment (evaluation of performance on unseen subj)\n",
    "(results will be stored in experiment folder)\n",
    "`aftrain(target_sub, model, af_params, metadataset: MetaDataset, iterations=1, length=50, logging=False,\n",
    "            experiment_name='experiment', last_layer=False)`\n",
    "- **iterations**: int, number of fine-tuning experiments for each subject (allows you to get average statistics)\n",
    "- **length** : int, max number of datapoints for fine-tuning\n",
    "- **last_layer** : boolean, if used supported NN (freezes input layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68281eb8fd5d83",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "er.aftrain(target_sub=targ, model=model,\n",
    "           af_params=af_params, metadataset=dataset,\n",
    "           length=65, # max number of datapoints which will be used for fine-tuning\n",
    "           iterations=5, # number of similar experiments with different random seed to get the mean stats\n",
    "           experiment_name=experiment_name, last_layer=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
